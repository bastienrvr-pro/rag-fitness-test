{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä √âVALUATION COMPL√àTE RAG - Golden Dataset + M√©triques\n",
    "\n",
    "**Objectif** : Mesurer quantitativement la qualit√© du RAG\n",
    "\n",
    "**Composants** :\n",
    "1. Golden Dataset (v√©rit√© terrain)\n",
    "2. Recall@K (qualit√© retrieval)\n",
    "3. LLM-as-Judge (qualit√© g√©n√©ration)\n",
    "4. Monitoring (thumbs up/down)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ √âTAPE 1 : Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports OK\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ajouter src au path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.retriever import Retriever\n",
    "from src.chatbot import RAGChatbot\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ √âTAPE 2 : Cr√©er Golden Dataset\n",
    "\n",
    "**Golden Dataset** = Paires Question/R√©ponse + Documents sources valid√©es par expert\n",
    "\n",
    "Format :\n",
    "```json\n",
    "{\n",
    "  \"question\": \"...\",\n",
    "  \"expected_answer\": \"...\",\n",
    "  \"relevant_docs\": [\"schoenfeld_rom.pdf\", ...],\n",
    "  \"category\": \"nutrition\" | \"rom\" | \"volume\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Golden Dataset cr√©√© : 20 paires Q/R\n",
      "\n",
      "R√©partition cat√©gories :\n",
      "   nutrition: 7\n",
      "   volume: 5\n",
      "   out_of_scope: 3\n",
      "   rom: 5\n"
     ]
    }
   ],
   "source": [
    "# Golden Dataset - 20 paires Q/R\n",
    "GOLDEN_DATASET = [\n",
    "    # ========================================================================\n",
    "    # NUTRITION (6 questions)\n",
    "    # ========================================================================\n",
    "    {\n",
    "        \"question\": \"What is the optimal protein intake for muscle hypertrophy in resistance training?\",\n",
    "        \"expected_answer\": \"1.4-2.0 g/kg/day for active individuals, up to 2.3-3.1 g/kg lean mass during caloric restriction\",\n",
    "        \"relevant_docs\": [\"issn_protein_position.pdf\", \"helms_bodybuilding_nutrition.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much protein per meal for optimal muscle protein synthesis?\",\n",
    "        \"expected_answer\": \"20-40g per meal, containing 2-3g leucine\",\n",
    "        \"relevant_docs\": [\"issn_protein_position.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is creatine supplementation effective for muscle growth?\",\n",
    "        \"expected_answer\": \"Yes, creatine monohydrate increases lean mass and strength, 3-5g/day\",\n",
    "        \"relevant_docs\": [\"issn_protein_position.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the protein content of chicken breast?\",\n",
    "        \"expected_answer\": \"~22-30g protein per 100g depending on preparation\",\n",
    "        \"relevant_docs\": [\"ciqual_2020.xls\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Should protein intake increase during caloric deficit?\",\n",
    "        \"expected_answer\": \"Yes, up to 2.3-3.1 g/kg lean body mass to preserve muscle\",\n",
    "        \"relevant_docs\": [\"helms_bodybuilding_nutrition.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the best protein sources for vegetarians?\",\n",
    "        \"expected_answer\": \"Soy, legumes, quinoa - combine sources for complete amino acid profile\",\n",
    "        \"relevant_docs\": [\"ciqual_2020.xls\", \"issn_protein_position.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RANGE OF MOTION (5 questions)\n",
    "    # ========================================================================\n",
    "    {\n",
    "        \"question\": \"Does full range of motion improve muscle hypertrophy compared to partial ROM?\",\n",
    "        \"expected_answer\": \"Yes, full ROM produces greater muscle growth, especially in stretched position\",\n",
    "        \"relevant_docs\": [\"schoenfeld_rom_hypertrophy.pdf\"],\n",
    "        \"category\": \"rom\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What ROM is best for quadriceps hypertrophy?\",\n",
    "        \"expected_answer\": \"0-130¬∞ knee flexion (full ROM) superior to 50-100¬∞ partial ROM\",\n",
    "        \"relevant_docs\": [\"schoenfeld_rom_hypertrophy.pdf\"],\n",
    "        \"category\": \"rom\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does training at long muscle lengths improve hypertrophy?\",\n",
    "        \"expected_answer\": \"Yes, exercises emphasizing stretch position show greater growth\",\n",
    "        \"relevant_docs\": [\"schoenfeld_rom_hypertrophy.pdf\"],\n",
    "        \"category\": \"rom\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can partial ROM be beneficial for advanced lifters?\",\n",
    "        \"expected_answer\": \"Limited evidence, full ROM generally superior for hypertrophy\",\n",
    "        \"relevant_docs\": [\"schoenfeld_rom_hypertrophy.pdf\"],\n",
    "        \"category\": \"rom\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the mechanism for ROM effects on muscle growth?\",\n",
    "        \"expected_answer\": \"Increased mechanical tension, metabolic stress, and muscle damage at full ROM\",\n",
    "        \"relevant_docs\": [\"schoenfeld_rom_hypertrophy.pdf\"],\n",
    "        \"category\": \"rom\"\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAINING VOLUME (5 questions)\n",
    "    # ========================================================================\n",
    "    {\n",
    "        \"question\": \"What is the optimal training volume for muscle hypertrophy?\",\n",
    "        \"expected_answer\": \"10-20 sets per muscle per week, dose-response relationship\",\n",
    "        \"relevant_docs\": [\"bernardez_training_variables.pdf\"],\n",
    "        \"category\": \"volume\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is there a maximum effective training volume?\",\n",
    "        \"expected_answer\": \"Yes, beyond 20-25 sets/week may cause diminishing returns or overtraining\",\n",
    "        \"relevant_docs\": [\"bernardez_training_variables.pdf\"],\n",
    "        \"category\": \"volume\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does training frequency affect hypertrophy?\",\n",
    "        \"expected_answer\": \"2-3x per muscle per week optimal when total volume equated\",\n",
    "        \"relevant_docs\": [\"bernardez_training_variables.pdf\"],\n",
    "        \"category\": \"volume\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Should beginners train with high volume?\",\n",
    "        \"expected_answer\": \"No, start lower (5-10 sets/week), progressively increase\",\n",
    "        \"relevant_docs\": [\"bernardez_training_variables.pdf\"],\n",
    "        \"category\": \"volume\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the minimum effective volume for muscle growth?\",\n",
    "        \"expected_answer\": \"~10 sets per muscle per week, below this growth is suboptimal\",\n",
    "        \"relevant_docs\": [\"bernardez_training_variables.pdf\"],\n",
    "        \"category\": \"volume\"\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EDGE CASES (4 questions)\n",
    "    # ========================================================================\n",
    "    {\n",
    "        \"question\": \"What is the best time to train for muscle growth?\",\n",
    "        \"expected_answer\": \"N/A - Not in sources\",\n",
    "        \"relevant_docs\": [],\n",
    "        \"category\": \"out_of_scope\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How to create a beginner workout program?\",\n",
    "        \"expected_answer\": \"N/A - Not in sources\",\n",
    "        \"relevant_docs\": [],\n",
    "        \"category\": \"out_of_scope\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What supplements should I take?\",\n",
    "        \"expected_answer\": \"Creatine, protein powder - others limited evidence\",\n",
    "        \"relevant_docs\": [\"issn_protein_position.pdf\"],\n",
    "        \"category\": \"nutrition\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is cardio bad for muscle growth?\",\n",
    "        \"expected_answer\": \"N/A - Not in sources\",\n",
    "        \"relevant_docs\": [],\n",
    "        \"category\": \"out_of_scope\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Golden Dataset cr√©√© : {len(GOLDEN_DATASET)} paires Q/R\")\n",
    "print(f\"\\nR√©partition cat√©gories :\")\n",
    "for cat in set([q['category'] for q in GOLDEN_DATASET]):\n",
    "    count = len([q for q in GOLDEN_DATASET if q['category'] == cat])\n",
    "    print(f\"   {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ √âTAPE 3 : Sauvegarder Golden Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Golden Dataset sauvegard√© : c:\\RAG-Fitness-Test\\data\\golden_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder en JSON\n",
    "GOLDEN_PATH = Path.cwd().parent / \"data\" / \"golden_dataset.json\"\n",
    "\n",
    "with open(GOLDEN_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(GOLDEN_DATASET, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Golden Dataset sauvegard√© : {GOLDEN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç √âTAPE 4 : √âvaluer RETRIEVER (Recall@K, MRR, Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç √âVALUATION RETRIEVER\n",
      "================================================================================\n",
      "üîß Initialisation Retriever...\n",
      "   üì• Chargement mod√®le : BAAI/bge-large-en-v1.5\n",
      "   üíæ Connexion ChromaDB : c:\\RAG-Fitness-Test\\data\\processed\\chroma_db\n",
      "   ‚úÖ Collection 'fitness_knowledge_base' charg√©e : 1438 documents\n",
      "   üî§ Initialisation BM25...\n",
      "      ‚úÖ BM25 index√© : 1438 documents\n",
      "   üéØ Chargement Cross-Encoder...\n",
      "      ‚úÖ Cross-Encoder charg√©\n",
      "\n",
      "üìä M√âTRIQUES RETRIEVER\n",
      "   Recall@5     : 82.4% (le bon doc est dans top 5)\n",
      "   MRR          : 0.580 (position moyenne du bon doc)\n",
      "   Precision@5  : 51.8% (% docs pertinents dans top 5)\n",
      "\n",
      "   √âvalu√© sur 17 questions\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç √âVALUATION RETRIEVER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialiser retriever\n",
    "retriever = Retriever()\n",
    "\n",
    "# M√©triques\n",
    "recall_at_5 = []\n",
    "mrr_scores = []\n",
    "precision_at_5 = []\n",
    "\n",
    "for item in GOLDEN_DATASET:\n",
    "    query = item['question']\n",
    "    relevant_docs = set(item['relevant_docs'])\n",
    "    \n",
    "    # Skip si pas de docs attendus (out of scope)\n",
    "    if not relevant_docs:\n",
    "        continue\n",
    "    \n",
    "    # Retrieval\n",
    "    #results = retriever.search(query=query, top_k=5)\n",
    "    results = retriever.hybrid_search(query=query, top_k=5, retrieve_k=20, alpha=0.5)\n",
    "\n",
    "\n",
    "    retrieved_docs = [r['source'] for r in results]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Recall@5 : Est-ce qu'au moins 1 doc pertinent est dans top 5 ?\n",
    "    # ========================================================================\n",
    "    has_relevant = any(doc in relevant_docs for doc in retrieved_docs)\n",
    "    recall_at_5.append(1.0 if has_relevant else 0.0)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MRR (Mean Reciprocal Rank) : √Ä quelle position le 1er doc pertinent ?\n",
    "    # ========================================================================\n",
    "    reciprocal_rank = 0.0\n",
    "    for rank, doc in enumerate(retrieved_docs, 1):\n",
    "        if doc in relevant_docs:\n",
    "            reciprocal_rank = 1.0 / rank\n",
    "            break\n",
    "    mrr_scores.append(reciprocal_rank)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Precision@5 : Combien de docs pertinents dans top 5 ?\n",
    "    # ========================================================================\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_docs if doc in relevant_docs)\n",
    "    precision_at_5.append(relevant_retrieved / 5.0)\n",
    "\n",
    "# Calculer moyennes\n",
    "avg_recall = sum(recall_at_5) / len(recall_at_5) * 100\n",
    "avg_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "avg_precision = sum(precision_at_5) / len(precision_at_5) * 100\n",
    "\n",
    "print(f\"\\nüìä M√âTRIQUES RETRIEVER\")\n",
    "print(f\"   Recall@5     : {avg_recall:.1f}% (le bon doc est dans top 5)\")\n",
    "print(f\"   MRR          : {avg_mrr:.3f} (position moyenne du bon doc)\")\n",
    "print(f\"   Precision@5  : {avg_precision:.1f}% (% docs pertinents dans top 5)\")\n",
    "print(f\"\\n   √âvalu√© sur {len(recall_at_5)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ √âTAPE 5 : √âvaluer GENERATOR (LLM-as-Judge)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT** : N√©cessite Ollama en cours d'ex√©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ √âVALUATION GENERATOR (LLM-as-Judge)\n",
      "================================================================================\n",
      "‚ö†Ô∏è Cela va prendre 5-10 minutes (g√©n√©ration + √©valuation)...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ü§ñ INITIALISATION CHATBOT RAG\n",
      "================================================================================\n",
      "üîß Initialisation Retriever...\n",
      "   üì• Chargement mod√®le : BAAI/bge-large-en-v1.5\n",
      "   üíæ Connexion ChromaDB : c:\\RAG-Fitness-Test\\data\\processed\\chroma_db\n",
      "   ‚úÖ Collection 'fitness_knowledge_base' charg√©e : 1438 documents\n",
      "   üî§ Initialisation BM25...\n",
      "      ‚úÖ BM25 index√© : 1438 documents\n",
      "   üéØ Chargement Cross-Encoder...\n",
      "      ‚úÖ Cross-Encoder charg√©\n",
      "\n",
      "üîç V√©rification Ollama (http://localhost:11434)...\n",
      "   ‚úÖ Ollama disponible\n",
      "   üß† Mod√®le : llama3.2:3b\n",
      "\n",
      "‚úÖ Chatbot pr√™t !\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[1/5] What is the optimal protein intake for muscle hypertrophy in...\n",
      "   Faithfulness: 5/5\n",
      "   Completeness: 4/5\n",
      "   Relevance: 4/5\n",
      "\n",
      "[2/5] Does full range of motion improve muscle hypertrophy compare...\n",
      "   Faithfulness: 5/5\n",
      "   Completeness: 4/5\n",
      "   Relevance: 5/5\n",
      "\n",
      "[3/5] What is the optimal training volume for muscle hypertrophy?...\n",
      "   ‚ö†Ô∏è Erreur √©valuation : Invalid control character at: line 5 column 21 (char 82)\n",
      "   Faithfulness: 0/5\n",
      "   Completeness: 0/5\n",
      "   Relevance: 0/5\n",
      "\n",
      "[4/5] What is the best time to train for muscle growth?...\n",
      "   ‚ö†Ô∏è Erreur √©valuation : Expecting ',' delimiter: line 10 column 479 (char 1284)\n",
      "   Faithfulness: 0/5\n",
      "   Completeness: 0/5\n",
      "   Relevance: 0/5\n",
      "\n",
      "[5/5] Is creatine supplementation effective for muscle growth?...\n",
      "   Faithfulness: 4/5\n",
      "   Completeness: 3/5\n",
      "   Relevance: 5/5\n",
      "\n",
      "================================================================================\n",
      "üìä SCORES LLM-AS-JUDGE (Moyennes)\n",
      "================================================================================\n",
      "   Faithfulness : 2.80/5\n",
      "   Completeness : 2.20/5\n",
      "   Relevance    : 2.80/5\n",
      "\n",
      "   √âvalu√© sur 5 questions\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ √âVALUATION GENERATOR (LLM-as-Judge)\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö†Ô∏è Cela va prendre 5-10 minutes (g√©n√©ration + √©valuation)...\\n\")\n",
    "\n",
    "# Initialiser chatbot\n",
    "chatbot = RAGChatbot()\n",
    "\n",
    "# Prompt pour LLM-as-Judge\n",
    "JUDGE_PROMPT = \"\"\"Tu es un √©valuateur expert de syst√®mes RAG.\n",
    "\n",
    "√âvalue cette r√©ponse selon 3 crit√®res (1-5) :\n",
    "\n",
    "1. **Faithfulness** (Fid√©lit√©) : La r√©ponse ne contient QUE des infos du contexte ?\n",
    "   - 5 : 100% fid√®le, aucune hallucination\n",
    "   - 1 : Invente des infos, hallucinations\n",
    "\n",
    "2. **Completeness** (Compl√©tude) : R√©pond-elle compl√®tement √† la question ?\n",
    "   - 5 : R√©ponse compl√®te, tous aspects couverts\n",
    "   - 1 : R√©ponse partielle ou √©vasive\n",
    "\n",
    "3. **Relevance** (Pertinence) : Est-elle concise et utile ?\n",
    "   - 5 : Concise, directe, utile\n",
    "   - 1 : Verbeuse, hors-sujet\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "R√©ponse : {answer}\n",
    "\n",
    "R√©ponds UNIQUEMENT avec ce format JSON :\n",
    "{{\n",
    "  \"faithfulness\": X,\n",
    "  \"completeness\": X,\n",
    "  \"relevance\": X,\n",
    "  \"justification\": \"...\"\n",
    "}}\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def llm_as_judge(question: str, answer: str) -> dict:\n",
    "    \"\"\"√âvalue une r√©ponse avec LLM-as-Judge\"\"\"\n",
    "    \n",
    "    prompt = JUDGE_PROMPT.format(question=question, answer=answer)\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3.2:3b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": 0.1,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        text = result.get('response', '')\n",
    "        \n",
    "        # Nettoyer JSON\n",
    "        text = text.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        # Parser\n",
    "        scores = json.loads(text)\n",
    "        return scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur √©valuation : {e}\")\n",
    "        return {\n",
    "            \"faithfulness\": 0,\n",
    "            \"completeness\": 0,\n",
    "            \"relevance\": 0,\n",
    "            \"justification\": \"Error\"\n",
    "        }\n",
    "\n",
    "# √âvaluer √©chantillon (5 questions pour rapidit√©)\n",
    "sample_questions = [\n",
    "    GOLDEN_DATASET[0],  # Protein\n",
    "    GOLDEN_DATASET[6],  # ROM\n",
    "    GOLDEN_DATASET[11], # Volume\n",
    "    GOLDEN_DATASET[16], # Out of scope\n",
    "    GOLDEN_DATASET[2]   # Creatine\n",
    "]\n",
    "\n",
    "judge_scores = {\n",
    "    'faithfulness': [],\n",
    "    'completeness': [],\n",
    "    'relevance': []\n",
    "}\n",
    "\n",
    "for i, item in enumerate(sample_questions, 1):\n",
    "    print(f\"\\n[{i}/{len(sample_questions)}] {item['question'][:60]}...\")\n",
    "    \n",
    "    # G√©n√©rer r√©ponse\n",
    "    result = chatbot.answer(item['question'], top_k=3)\n",
    "    answer = result['answer']\n",
    "    \n",
    "    # √âvaluer\n",
    "    scores = llm_as_judge(item['question'], answer)\n",
    "    \n",
    "    judge_scores['faithfulness'].append(scores['faithfulness'])\n",
    "    judge_scores['completeness'].append(scores['completeness'])\n",
    "    judge_scores['relevance'].append(scores['relevance'])\n",
    "    \n",
    "    print(f\"   Faithfulness: {scores['faithfulness']}/5\")\n",
    "    print(f\"   Completeness: {scores['completeness']}/5\")\n",
    "    print(f\"   Relevance: {scores['relevance']}/5\")\n",
    "\n",
    "# Moyennes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SCORES LLM-AS-JUDGE (Moyennes)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Faithfulness : {sum(judge_scores['faithfulness'])/len(judge_scores['faithfulness']):.2f}/5\")\n",
    "print(f\"   Completeness : {sum(judge_scores['completeness'])/len(judge_scores['completeness']):.2f}/5\")\n",
    "print(f\"   Relevance    : {sum(judge_scores['relevance'])/len(judge_scores['relevance']):.2f}/5\")\n",
    "print(f\"\\n   √âvalu√© sur {len(sample_questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëç √âTAPE 6 : Syst√®me de Monitoring (Thumbs Up/Down)\n",
    "\n",
    "Code √† int√©grer dans Gradio pour feedback utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç CODE MONITORING\n",
      "================================================================================\n",
      "\n",
      "# ============================================================================\n",
      "# MONITORING - Feedback Utilisateur\n",
      "# ============================================================================\n",
      "\n",
      "import json\n",
      "from datetime import datetime\n",
      "from pathlib import Path\n",
      "\n",
      "FEEDBACK_LOG = Path(\"data/feedback_log.jsonl\")\n",
      "\n",
      "def log_feedback(question: str, answer: str, feedback: str, sources: list):\n",
      "    \"\"\"\n",
      "    Enregistre feedback utilisateur\n",
      "\n",
      "    Args:\n",
      "        question: Question pos√©e\n",
      "        answer: R√©ponse g√©n√©r√©e\n",
      "        feedback: \"positive\" ou \"negative\"\n",
      "        sources: Liste documents sources\n",
      "    \"\"\"\n",
      "\n",
      "    entry = {\n",
      "        \"timestamp\": datetime.now().isoformat(),\n",
      "        \"question\": question,\n",
      "        \"answer\": answer[:200],  # Tronqu√©\n",
      "        \"feedback\": feedback,\n",
      "        \"sources\": [s[\"source\"] for s in sources],\n",
      "        \"num_sources\": len(sources)\n",
      "    }\n",
      "\n",
      "    # Append to JSONL\n",
      "    with open(FEEDBACK_LOG, \"a\", encoding=\"utf-8\") as f:\n",
      "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
      "\n",
      "\n",
      "# Dans l'interface Gradio, ajouter boutons feedback :\n",
      "\n",
      "with gr.Row():\n",
      "    thumbs_up = gr.Button(\"üëç Bonne r√©ponse\")\n",
      "    thumbs_down = gr.Button(\"üëé Mauvaise r√©ponse\")\n",
      "\n",
      "# Event handlers\n",
      "thumbs_up.click(\n",
      "    fn=lambda q, a, s: log_feedback(q, a, \"positive\", s),\n",
      "    inputs=[last_question, last_answer, last_sources]\n",
      ")\n",
      "\n",
      "thumbs_down.click(\n",
      "    fn=lambda q, a, s: log_feedback(q, a, \"negative\", s),\n",
      "    inputs=[last_question, last_answer, last_sources]\n",
      ")\n",
      "\n",
      "\n",
      "‚úÖ √Ä int√©grer dans app.py pour feedback utilisateur\n"
     ]
    }
   ],
   "source": [
    "# Code pour app.py\n",
    "monitoring_code = '''\n",
    "# ============================================================================\n",
    "# MONITORING - Feedback Utilisateur\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "FEEDBACK_LOG = Path(\"data/feedback_log.jsonl\")\n",
    "\n",
    "def log_feedback(question: str, answer: str, feedback: str, sources: list):\n",
    "    \"\"\"\n",
    "    Enregistre feedback utilisateur\n",
    "    \n",
    "    Args:\n",
    "        question: Question pos√©e\n",
    "        answer: R√©ponse g√©n√©r√©e\n",
    "        feedback: \"positive\" ou \"negative\"\n",
    "        sources: Liste documents sources\n",
    "    \"\"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"question\": question,\n",
    "        \"answer\": answer[:200],  # Tronqu√©\n",
    "        \"feedback\": feedback,\n",
    "        \"sources\": [s[\"source\"] for s in sources],\n",
    "        \"num_sources\": len(sources)\n",
    "    }\n",
    "    \n",
    "    # Append to JSONL\n",
    "    with open(FEEDBACK_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\\\n\")\n",
    "\n",
    "\n",
    "# Dans l\\'interface Gradio, ajouter boutons feedback :\n",
    "\n",
    "with gr.Row():\n",
    "    thumbs_up = gr.Button(\"üëç Bonne r√©ponse\")\n",
    "    thumbs_down = gr.Button(\"üëé Mauvaise r√©ponse\")\n",
    "\n",
    "# Event handlers\n",
    "thumbs_up.click(\n",
    "    fn=lambda q, a, s: log_feedback(q, a, \"positive\", s),\n",
    "    inputs=[last_question, last_answer, last_sources]\n",
    ")\n",
    "\n",
    "thumbs_down.click(\n",
    "    fn=lambda q, a, s: log_feedback(q, a, \"negative\", s),\n",
    "    inputs=[last_question, last_answer, last_sources]\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"üëç CODE MONITORING\")\n",
    "print(\"=\"*80)\n",
    "print(monitoring_code)\n",
    "print(\"\\n‚úÖ √Ä int√©grer dans app.py pour feedback utilisateur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä √âTAPE 7 : Analyse Feedback (Monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonction analyze_feedback() cr√©√©e\n",
      "\n",
      "Utilisation :\n",
      "   analyze_feedback(Path('data/feedback_log.jsonl'))\n"
     ]
    }
   ],
   "source": [
    "# Fonction d'analyse feedback\n",
    "def analyze_feedback(feedback_log_path: Path):\n",
    "    \"\"\"\n",
    "    Analyse le fichier de feedback utilisateur\n",
    "    \"\"\"\n",
    "    \n",
    "    if not feedback_log_path.exists():\n",
    "        print(\"‚ö†Ô∏è Pas encore de feedback utilisateur\")\n",
    "        return\n",
    "    \n",
    "    # Charger logs\n",
    "    feedbacks = []\n",
    "    with open(feedback_log_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            feedbacks.append(json.loads(line))\n",
    "    \n",
    "    if not feedbacks:\n",
    "        print(\"‚ö†Ô∏è Aucun feedback enregistr√©\")\n",
    "        return\n",
    "    \n",
    "    # Statistiques\n",
    "    total = len(feedbacks)\n",
    "    positive = sum(1 for f in feedbacks if f['feedback'] == 'positive')\n",
    "    negative = total - positive\n",
    "    \n",
    "    satisfaction = positive / total * 100\n",
    "    \n",
    "    print(\"\\nüìä STATISTIQUES FEEDBACK UTILISATEUR\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   Total feedbacks : {total}\")\n",
    "    print(f\"   üëç Positifs     : {positive} ({positive/total*100:.1f}%)\")\n",
    "    print(f\"   üëé N√©gatifs     : {negative} ({negative/total*100:.1f}%)\")\n",
    "    print(f\"\\n   Taux satisfaction : {satisfaction:.1f}%\")\n",
    "    \n",
    "    # Questions les plus probl√©matiques\n",
    "    negative_questions = [f for f in feedbacks if f['feedback'] == 'negative']\n",
    "    \n",
    "    if negative_questions:\n",
    "        print(\"\\n‚ö†Ô∏è QUESTIONS PROBL√âMATIQUES (feedback n√©gatif) :\")\n",
    "        for i, f in enumerate(negative_questions[:5], 1):\n",
    "            print(f\"   {i}. {f['question'][:60]}...\")\n",
    "            print(f\"      Sources : {', '.join(f['sources'])}\")\n",
    "\n",
    "# Exemple utilisation (une fois feedback collect√©)\n",
    "print(\"‚úÖ Fonction analyze_feedback() cr√©√©e\")\n",
    "print(\"\\nUtilisation :\")\n",
    "print(\"   analyze_feedback(Path('data/feedback_log.jsonl'))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã √âTAPE 8 : Rapport Final\n",
    "\n",
    "G√©n√©rer rapport d'√©valuation complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rapport d'√©valuation g√©n√©r√©\n",
      "   üìÑ c:\\RAG-Fitness-Test\\EVALUATION_REPORT.md\n",
      "\n",
      "# üìä RAPPORT √âVALUATION RAG FITNESS\n",
      "\n",
      "Date : 2025-12-23 13:35\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ GOLDEN DATASET\n",
      "\n",
      "- **Total questions** : 20\n",
      "- **Cat√©gories** :\n",
      "  - Nutrition : 7 questions\n",
      "  - ROM : 5 questions\n",
      "  - Volume : 5 questions\n",
      "  - Out of scope : 3 questions\n",
      "\n",
      "---\n",
      "\n",
      "## üîç M√âTRIQUES RETRIEVER\n",
      "\n",
      "| M√©trique | Score | Interpr√©tation |\n",
      "|----------|-------|----------------|\n",
      "| **Recall@5** | 82.4% | Le bon document est trouv√© dans 82% des cas |\n",
      "| **MRR** | 0.580 | Position moyenne du bon doc : 1.7 |\n",
      "| **Precision@5** | 51.8% | 52% des docs retourn√©s sont pertinents |\n",
      "\n",
      "**Interpr√©tation** :\n",
      "- Recall > 80% : ‚úÖ Excellent\n",
      "- Recall 60-80% : ‚ö†Ô∏è Acceptable\n",
      "- Recall < 60% : ‚ùå √Ä am√©liorer\n",
      "\n",
      "---\n",
      "\n",
      "## ü§ñ M√âTRIQUES GENERATOR (LLM-as-Judge)\n",
      "\n",
      "| Crit√®re | Score Moyen | Cible |\n",
      "|---------|-------------|-------|\n",
      "| **Faithfulness** | 2.80/5 | > 4.0 |\n",
      "| **Completeness** | 2.20/5 | > 3.5 |\n",
      "| **Relevance** | 2.80/5 | > 4.0 |\n",
      "\n",
      "**√âchantillon √©valu√©** : 5 questions\n",
      "\n",
      "---\n",
      "\n",
      "## üëç MONITORING (Feedback Utilisateur)\n",
      "\n",
      "**Statut** : √Ä impl√©menter dans app.py\n",
      "\n",
      "**Fonctionnalit√©s** :\n",
      "- Boutons üëç üëé dans interface\n",
      "- Log automatique dans `feedback_log.jsonl`\n",
      "- Analyse p√©riodique avec `analyze_feedback()`\n",
      "\n",
      "**KPI √† suivre** :\n",
      "- Taux satisfaction (cible > 80%)\n",
      "- Questions probl√©matiques\n",
      "- Drift detection (changement sujets)\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ RECOMMANDATIONS\n",
      "\n",
      "### Si Recall@5 < 70%\n",
      "1. Impl√©menter Hybrid Search (BM25 + Dense)\n",
      "2. Ajouter Re-ranking Cross-Encoder\n",
      "3. Am√©liorer chunking (Semantic)\n",
      "\n",
      "### Si Faithfulness < 4.0\n",
      "1. Renforcer prompt syst√®me\n",
      "2. R√©duire temp√©rature LLM\n",
      "3. Filtrer contexte (top 3 au lieu de 5)\n",
      "\n",
      "### Si Completeness < 3.5\n",
      "1. Augmenter top_k retrieval\n",
      "2. Augmenter max_tokens LLM\n",
      "3. Am√©liorer formulation prompt\n",
      "\n",
      "---\n",
      "\n",
      "## üìÅ FICHIERS G√âN√âR√âS\n",
      "\n",
      "- `data/golden_dataset.json` : V√©rit√© terrain (20 Q/R)\n",
      "- `data/feedback_log.jsonl` : Logs feedback utilisateur\n",
      "- `notebooks/04_evaluation.ipynb` : Code √©valuation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Cr√©er rapport\n",
    "report = f\"\"\"# üìä RAPPORT √âVALUATION RAG FITNESS\n",
    "\n",
    "Date : {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ GOLDEN DATASET\n",
    "\n",
    "- **Total questions** : {len(GOLDEN_DATASET)}\n",
    "- **Cat√©gories** :\n",
    "  - Nutrition : 7 questions\n",
    "  - ROM : 5 questions\n",
    "  - Volume : 5 questions\n",
    "  - Out of scope : 3 questions\n",
    "\n",
    "---\n",
    "\n",
    "## üîç M√âTRIQUES RETRIEVER\n",
    "\n",
    "| M√©trique | Score | Interpr√©tation |\n",
    "|----------|-------|----------------|\n",
    "| **Recall@5** | {avg_recall:.1f}% | Le bon document est trouv√© dans {avg_recall:.0f}% des cas |\n",
    "| **MRR** | {avg_mrr:.3f} | Position moyenne du bon doc : {1/avg_mrr:.1f} |\n",
    "| **Precision@5** | {avg_precision:.1f}% | {avg_precision:.0f}% des docs retourn√©s sont pertinents |\n",
    "\n",
    "**Interpr√©tation** :\n",
    "- Recall > 80% : ‚úÖ Excellent\n",
    "- Recall 60-80% : ‚ö†Ô∏è Acceptable\n",
    "- Recall < 60% : ‚ùå √Ä am√©liorer\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ M√âTRIQUES GENERATOR (LLM-as-Judge)\n",
    "\n",
    "| Crit√®re | Score Moyen | Cible |\n",
    "|---------|-------------|-------|\n",
    "| **Faithfulness** | {sum(judge_scores['faithfulness'])/len(judge_scores['faithfulness']):.2f}/5 | > 4.0 |\n",
    "| **Completeness** | {sum(judge_scores['completeness'])/len(judge_scores['completeness']):.2f}/5 | > 3.5 |\n",
    "| **Relevance** | {sum(judge_scores['relevance'])/len(judge_scores['relevance']):.2f}/5 | > 4.0 |\n",
    "\n",
    "**√âchantillon √©valu√©** : 5 questions\n",
    "\n",
    "---\n",
    "\n",
    "## üëç MONITORING (Feedback Utilisateur)\n",
    "\n",
    "**Statut** : √Ä impl√©menter dans app.py\n",
    "\n",
    "**Fonctionnalit√©s** :\n",
    "- Boutons üëç üëé dans interface\n",
    "- Log automatique dans `feedback_log.jsonl`\n",
    "- Analyse p√©riodique avec `analyze_feedback()`\n",
    "\n",
    "**KPI √† suivre** :\n",
    "- Taux satisfaction (cible > 80%)\n",
    "- Questions probl√©matiques\n",
    "- Drift detection (changement sujets)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ RECOMMANDATIONS\n",
    "\n",
    "### Si Recall@5 < 70%\n",
    "1. Impl√©menter Hybrid Search (BM25 + Dense)\n",
    "2. Ajouter Re-ranking Cross-Encoder\n",
    "3. Am√©liorer chunking (Semantic)\n",
    "\n",
    "### Si Faithfulness < 4.0\n",
    "1. Renforcer prompt syst√®me\n",
    "2. R√©duire temp√©rature LLM\n",
    "3. Filtrer contexte (top 3 au lieu de 5)\n",
    "\n",
    "### Si Completeness < 3.5\n",
    "1. Augmenter top_k retrieval\n",
    "2. Augmenter max_tokens LLM\n",
    "3. Am√©liorer formulation prompt\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ FICHIERS G√âN√âR√âS\n",
    "\n",
    "- `data/golden_dataset.json` : V√©rit√© terrain (20 Q/R)\n",
    "- `data/feedback_log.jsonl` : Logs feedback utilisateur\n",
    "- `notebooks/04_evaluation.ipynb` : Code √©valuation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder rapport\n",
    "REPORT_PATH = Path.cwd().parent / \"EVALUATION_REPORT.md\"\n",
    "\n",
    "with open(REPORT_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"‚úÖ Rapport d'√©valuation g√©n√©r√©\")\n",
    "print(f\"   üìÑ {REPORT_PATH}\")\n",
    "print(\"\\n\" + report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ CONCLUSION\n",
    "\n",
    "**√âvaluation compl√®te impl√©ment√©e !**\n",
    "\n",
    "**Composants cr√©√©s** :\n",
    "1. ‚úÖ Golden Dataset (20 Q/R)\n",
    "2. ‚úÖ M√©triques Retriever (Recall, MRR, Precision)\n",
    "3. ‚úÖ LLM-as-Judge (Faithfulness, Completeness, Relevance)\n",
    "4. ‚úÖ Code Monitoring (Thumbs up/down)\n",
    "5. ‚úÖ Rapport d'√©valuation\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "1. Int√©grer monitoring dans app.py\n",
    "2. Suivre m√©triques en production\n",
    "3. It√©rer selon feedback utilisateur\n",
    "\n",
    "**Fr√©quence recommand√©e** :\n",
    "- √âvaluation Golden Dataset : Mensuelle\n",
    "- Analyse feedback : Hebdomadaire\n",
    "- Review m√©triques : Quotidienne (dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
