{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š RAG FITNESS - DATA INGESTION PIPELINE V2\n",
    "\n",
    "**Version 2 : Avec Metadata Enrichies (PHASE 1 de la fiche RAG)**\n",
    "\n",
    "## ğŸ¯ AmÃ©liorations de cette version :\n",
    "\n",
    "âœ… **PHASE 1 - Metadata Extraction** (selon ta fiche RAG)\n",
    "- Date (AnnÃ©e publication)\n",
    "- Auteur (Liste auteurs)\n",
    "- Journal / Source\n",
    "- Page (NumÃ©ro de page dans le PDF)\n",
    "- Section (Introduction, Methods, Results, etc.)\n",
    "\n",
    "**BÃ©nÃ©fices** :\n",
    "- ğŸ” Filtrage temporel (Ã©viter \"Stale Data\")\n",
    "- ğŸ“š Citations acadÃ©miques propres\n",
    "- ğŸ¯ Contextualisation des chunks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Ã‰TAPE 1 : Import des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Toutes les librairies importÃ©es avec succÃ¨s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Extraction PDF\n",
    "from pypdf import PdfReader\n",
    "import pymupdf  # PyMuPDF\n",
    "\n",
    "# Text splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector store\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "print(\"âœ… Toutes les librairies importÃ©es avec succÃ¨s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Ã‰TAPE 2 : Configuration des Chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Dossier de base : C:\\RAG-Fitness-Test\n",
      "ğŸ“ DonnÃ©es sources : C:\\RAG-Fitness-Test\\data\\pdfs\n",
      "ğŸ“ ChromaDB : C:\\RAG-Fitness-Test\\data\\processed\\chroma_db\n",
      "\n",
      "ğŸ“„ PDFs Ã  traiter : 4\n"
     ]
    }
   ],
   "source": [
    "# Chemins des donnÃ©es\n",
    "BASE_DIR = Path(r\"C:\\RAG-Fitness-Test\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"pdfs\"\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "CHROMA_DIR = BASE_DIR / \"data\" / \"processed\" / \"chroma_db\"\n",
    "\n",
    "# CrÃ©er les dossiers si nÃ©cessaire\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Liste des fichiers\n",
    "PDF_FILES = [\n",
    "    \"helms_bodybuilding_nutrition.pdf\",\n",
    "    \"issn_protein_position.pdf\",\n",
    "    \"schoenfeld_rom_hypertrophy.pdf\",\n",
    "    \"bernardez_training_variables.pdf\"\n",
    "]\n",
    "\n",
    "EXCEL_FILE = \"ciqual_2020.xls\"\n",
    "\n",
    "print(f\"ğŸ“ Dossier de base : {BASE_DIR}\")\n",
    "print(f\"ğŸ“ DonnÃ©es sources : {DATA_DIR}\")\n",
    "print(f\"ğŸ“ ChromaDB : {CHROMA_DIR}\")\n",
    "print(f\"\\nğŸ“„ PDFs Ã  traiter : {len(PDF_FILES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• Ã‰TAPE 2B : MODULE D'EXTRACTION METADATA\n",
    "\n",
    "### ğŸ”— Lien avec ta Fiche RAG - PHASE 1\n",
    "\n",
    "**Citation de ta fiche** :\n",
    "> *\"Metadata Extraction : Date, Auteur, Version. Crucial pour le filtrage temporel (Ã©viter la 'Stale Data').\"*\n",
    "\n",
    "### ğŸ“‹ Ce que fait ce module :\n",
    "\n",
    "1. **`extract_authors()`** â†’ Extrait les auteurs (ex: \"Helms ER, Aragon AA\")\n",
    "   - MÃ©thode 1 : Metadata PDF natives\n",
    "   - MÃ©thode 2 : Pattern matching dans premiÃ¨re page\n",
    "   - MÃ©thode 3 : Fallback sur patterns alternatifs\n",
    "\n",
    "2. **`extract_year()`** â†’ Extrait l'annÃ©e (ex: 2014)\n",
    "   - MÃ©thode 1 : Metadata PDF (creationDate)\n",
    "   - MÃ©thode 2 : Cherche pattern \"2014\" dans texte\n",
    "   - MÃ©thode 3 : Pattern \"(2014)\" bibliographique\n",
    "\n",
    "3. **`extract_journal()`** â†’ Extrait nom journal (ex: \"Journal of ISSN\")\n",
    "   - Patterns fitness/nutrition connus\n",
    "   - Pattern gÃ©nÃ©rique \"Journal of X\"\n",
    "\n",
    "4. **`detect_section()`** â†’ DÃ©tecte section article (Introduction, Methods, Results)\n",
    "   - Permet de contextualiser le chunk\n",
    "   - Utile pour filtrer par type de contenu\n",
    "\n",
    "5. **`estimate_page_number()`** â†’ Trouve la page du chunk dans le PDF\n",
    "   - Permet citation prÃ©cise\n",
    "   - Utile pour vÃ©rification sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Module d'extraction metadata chargÃ©\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODULE D'EXTRACTION METADATA\n",
    "# ============================================================================\n",
    "\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    \"\"\"\n",
    "    Extrait mÃ©tadonnÃ©es enrichies d'un PDF scientifique\n",
    "    \n",
    "    POURQUOI ? (Selon ta fiche PHASE 1)\n",
    "    - Filtrage temporel : Ã‰viter Ã©tudes obsolÃ¨tes\n",
    "    - Citations : CrÃ©dibilitÃ© acadÃ©mique\n",
    "    - Contextualisation : Savoir d'oÃ¹ vient l'info\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    meta = doc.metadata\n",
    "    first_page_text = doc[0].get_text()\n",
    "    \n",
    "    metadata = {\n",
    "        'authors': extract_authors(first_page_text, meta),\n",
    "        'year': extract_year(first_page_text, meta),\n",
    "        'journal': extract_journal(first_page_text),\n",
    "        'title': extract_title(first_page_text, meta),\n",
    "        'total_pages': len(doc)\n",
    "    }\n",
    "    \n",
    "    doc.close()\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_authors(text, meta):\n",
    "    \"\"\"\n",
    "    Extrait liste d'auteurs du PDF\n",
    "    \n",
    "    LOGIQUE :\n",
    "    - Essaie 3 mÃ©thodes par ordre de fiabilitÃ©\n",
    "    - S'arrÃªte Ã  la premiÃ¨re qui fonctionne\n",
    "    - Retourne ['Unknown'] si aucune ne marche\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    \n",
    "    # MÃ©thode 1: Metadata PDF natives\n",
    "    if meta.get('author'):\n",
    "        authors_str = meta.get('author')\n",
    "        # SÃ©parer par virgules ou \"and\"\n",
    "        authors = re.split(r',|\\s+and\\s+', authors_str)\n",
    "        authors = [a.strip() for a in authors if a.strip()]\n",
    "    \n",
    "    # MÃ©thode 2: Pattern scientifique \"Helms ER, Aragon AA\"\n",
    "    if not authors:\n",
    "        pattern = r'([A-Z][a-z]+\\s+[A-Z]{1,3})'\n",
    "        matches = re.findall(pattern, text[:1000])  # Premier 1000 chars\n",
    "        if matches:\n",
    "            authors = matches[:5]  # Max 5 auteurs\n",
    "    \n",
    "    # MÃ©thode 3: Pattern \"Eric Helms1, Alan Aragon2\"\n",
    "    if not authors:\n",
    "        pattern = r'([A-Z][a-z]+\\s+[A-Z][a-z]+)[\\d,]'\n",
    "        matches = re.findall(pattern, text[:1000])\n",
    "        if matches:\n",
    "            authors = matches[:5]\n",
    "    \n",
    "    return authors if authors else [\"Unknown\"]\n",
    "\n",
    "\n",
    "def extract_year(text, meta):\n",
    "    \"\"\"\n",
    "    Extrait annÃ©e de publication\n",
    "    \n",
    "    IMPORTANCE (ta fiche) :\n",
    "    - Filtrage temporel : Ne garder que Ã©tudes rÃ©centes\n",
    "    - Exemple : min_year=2015 â†’ Exclure Ã©tudes < 2015\n",
    "    \"\"\"\n",
    "    # MÃ©thode 1: Metadata PDF\n",
    "    if meta.get('creationDate'):\n",
    "        try:\n",
    "            # Format: \"D:20140523...\"\n",
    "            date_str = meta.get('creationDate')\n",
    "            if date_str.startswith('D:'):\n",
    "                year = int(date_str[2:6])\n",
    "                if 1990 <= year <= datetime.now().year:\n",
    "                    return year\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # MÃ©thode 2: Pattern annÃ©es 2000-2025 dans texte\n",
    "    years = re.findall(r'\\b(20[0-2][0-9])\\b', text[:2000])\n",
    "    if years:\n",
    "        # Prendre l'annÃ©e la plus frÃ©quente\n",
    "        most_common = Counter(years).most_common(1)\n",
    "        return int(most_common[0][0])\n",
    "    \n",
    "    # MÃ©thode 3: Pattern \"(2014)\" bibliographique\n",
    "    pattern = r'\\((\\d{4})\\)'\n",
    "    matches = re.findall(pattern, text[:2000])\n",
    "    if matches:\n",
    "        years_list = [int(y) for y in matches if 1990 <= int(y) <= datetime.now().year]\n",
    "        if years_list:\n",
    "            return max(years_list)  # AnnÃ©e la plus rÃ©cente\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_journal(text):\n",
    "    \"\"\"\n",
    "    Extrait nom du journal\n",
    "    \n",
    "    UTILITÃ‰ :\n",
    "    - CrÃ©dibilitÃ© : ISSN > blog inconnu\n",
    "    - Citations : \"Selon Journal of ISSN...\"\n",
    "    \"\"\"\n",
    "    journal_patterns = [\n",
    "        r'Journal of (the )?International Society of Sports Nutrition',\n",
    "        r'Journal of Strength and Conditioning Research',\n",
    "        r'Sports Medicine',\n",
    "        r'European Journal of Applied Physiology',\n",
    "        r'Frontiers in \\w+',\n",
    "        r'SAGE Open Medicine'\n",
    "    ]\n",
    "    \n",
    "    for pattern in journal_patterns:\n",
    "        match = re.search(pattern, text[:1500], re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "    \n",
    "    # Pattern gÃ©nÃ©rique\n",
    "    generic = re.search(r'Journal of [A-Z][a-z\\s&]+', text[:1500])\n",
    "    if generic:\n",
    "        return generic.group(0)\n",
    "    \n",
    "    return \"Unknown Journal\"\n",
    "\n",
    "\n",
    "def extract_title(text, meta):\n",
    "    \"\"\"Extrait titre de l'article\"\"\"\n",
    "    if meta.get('title') and len(meta.get('title')) > 10:\n",
    "        return meta.get('title')\n",
    "    \n",
    "    lines = text.split('\\n')[:10]\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) > 20 and not line.isupper():\n",
    "            if not re.search(r'\\d{4}', line) and not re.search(r'Journal', line):\n",
    "                return line[:200]\n",
    "    \n",
    "    return \"Unknown Title\"\n",
    "\n",
    "\n",
    "def detect_section(chunk_text):\n",
    "    \"\"\"\n",
    "    DÃ©tecte section du document (Introduction, Methods, Results...)\n",
    "    \n",
    "    POURQUOI C'EST UTILE ?\n",
    "    - Contextualisation : \"Selon la section Methods...\"\n",
    "    - Filtrage : Ne garder que Results pour donnÃ©es chiffrÃ©es\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        'abstract': ['abstract', 'summary'],\n",
    "        'introduction': ['introduction', 'background'],\n",
    "        'methods': ['methods', 'methodology', 'materials and methods'],\n",
    "        'results': ['results', 'findings'],\n",
    "        'discussion': ['discussion'],\n",
    "        'conclusion': ['conclusion', 'conclusions'],\n",
    "        'references': ['references', 'bibliography']\n",
    "    }\n",
    "    \n",
    "    text_lower = chunk_text.lower()[:200]\n",
    "    \n",
    "    for section_name, keywords in sections.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                return section_name\n",
    "    \n",
    "    return 'body'  # Par dÃ©faut\n",
    "\n",
    "\n",
    "def estimate_page_number(doc, chunk_text, chunk_index, total_chunks):\n",
    "    \"\"\"\n",
    "    Estime numÃ©ro de page du chunk\n",
    "    \n",
    "    MÃ‰THODE :\n",
    "    1. Cherche le texte du chunk dans le PDF (prÃ©cis)\n",
    "    2. Sinon, estimation linÃ©aire (approximatif)\n",
    "    \"\"\"\n",
    "    search_text = chunk_text[:100].strip()  # 100 premiers caractÃ¨res\n",
    "    \n",
    "    # Chercher le texte dans chaque page\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        page_text = page.get_text()\n",
    "        if search_text in page_text:\n",
    "            return page_num\n",
    "    \n",
    "    # Fallback : Estimation linÃ©aire\n",
    "    pages_per_chunk = len(doc) / total_chunks\n",
    "    estimated_page = int(chunk_index * pages_per_chunk) + 1\n",
    "    return min(estimated_page, len(doc))\n",
    "\n",
    "\n",
    "print(\"âœ… Module d'extraction metadata chargÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– Ã‰TAPE 3 : Extraction du Texte des PDFs AVEC METADATA\n",
    "\n",
    "### ğŸ”„ DiffÃ©rence avec la version V1 :\n",
    "\n",
    "**AVANT (V1)** :\n",
    "```python\n",
    "pdf_contents[\"helms.pdf\"] = \"texte brut...\"\n",
    "```\n",
    "\n",
    "**APRÃˆS (V2)** :\n",
    "```python\n",
    "pdf_contents_with_meta[\"helms.pdf\"] = {\n",
    "    'text': \"texte brut...\",\n",
    "    'metadata': {\n",
    "        'authors': ['Eric Helms', 'Alan Aragon'],\n",
    "        'year': 2014,\n",
    "        'journal': 'Journal of ISSN',\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**BÃ‰NÃ‰FICE** : Les metadata seront propagÃ©es Ã  TOUS les chunks de ce PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– Extraction : helms_bodybuilding_nutrition.pdf...\n",
      "   ğŸ“‹ Auteurs: Open A, Eric R, Alan A\n",
      "   ğŸ“… AnnÃ©e: 2014\n",
      "   ğŸ“° Journal: Unknown Journal...\n",
      "   âœ… 124407 caractÃ¨res extraits\n",
      "\n",
      "ğŸ“– Extraction : issn_protein_position.pdf...\n",
      "   ğŸ“‹ Auteurs: Med C, International S, Sports N\n",
      "   ğŸ“… AnnÃ©e: 2015\n",
      "   ğŸ“° Journal: Unknown Journal...\n",
      "   âœ… 41102 caractÃ¨res extraits\n",
      "\n",
      "ğŸ“– Extraction : schoenfeld_rom_hypertrophy.pdf...\n",
      "   ğŸ“‹ Auteurs: Brad Schoenfeld\n",
      "   ğŸ“… AnnÃ©e: 2016\n",
      "   ğŸ“° Journal: Unknown Journal...\n",
      "   âœ… 615542 caractÃ¨res extraits\n",
      "\n",
      "ğŸ“– Extraction : bernardez_training_variables.pdf...\n",
      "   ğŸ“‹ Auteurs: Javier Raya-GonzaÌlez\n",
      "   ğŸ“… AnnÃ©e: 2022\n",
      "   ğŸ“° Journal: Frontiers in Sports...\n",
      "   âœ… 70511 caractÃ¨res extraits\n",
      "\n",
      "âœ… 4 PDFs traitÃ©s avec metadata enrichies\n"
     ]
    }
   ],
   "source": [
    "# Extraction de tous les PDFs avec metadata enrichies\n",
    "pdf_contents_with_meta = {}\n",
    "\n",
    "for pdf_file in PDF_FILES:\n",
    "    pdf_path = DATA_DIR / pdf_file\n",
    "    \n",
    "    if pdf_path.exists():\n",
    "        print(f\"\\nğŸ“– Extraction : {pdf_file}...\")\n",
    "        \n",
    "        # 1. Extraire metadata du PDF\n",
    "        pdf_meta = extract_pdf_metadata(pdf_path)\n",
    "        print(f\"   ğŸ“‹ Auteurs: {', '.join(pdf_meta['authors'][:3])}\")\n",
    "        print(f\"   ğŸ“… AnnÃ©e: {pdf_meta['year']}\")\n",
    "        print(f\"   ğŸ“° Journal: {pdf_meta['journal'][:50]}...\")\n",
    "        \n",
    "        # 2. Extraire texte\n",
    "        text = \"\"\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            text += f\"\\n\\n--- PAGE {page_num} ---\\n\\n\"\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        # 3. Stocker texte + metadata\n",
    "        pdf_contents_with_meta[pdf_file] = {\n",
    "            'text': text,\n",
    "            'metadata': pdf_meta\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… {len(text)} caractÃ¨res extraits\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   âŒ FICHIER INTROUVABLE : {pdf_file}\")\n",
    "\n",
    "print(f\"\\nâœ… {len(pdf_contents_with_meta)} PDFs traitÃ©s avec metadata enrichies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ã‰TAPE 4 : Traitement de la Table CIQUAL (InchangÃ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Chargement de la table CIQUAL...\n",
      "   âœ… 3186 aliments chargÃ©s\n",
      "   ğŸ“‹ Colonnes disponibles : 76\n",
      "   ğŸ” AperÃ§u des colonnes :\n",
      "      [0] alim_grp_code\n",
      "      [1] alim_ssgrp_code\n",
      "      [2] alim_ssssgrp_code\n",
      "      [3] alim_grp_nom_fr\n",
      "      [4] alim_ssgrp_nom_fr\n",
      "      [5] alim_ssssgrp_nom_fr\n",
      "      [6] alim_code\n",
      "      [7] alim_nom_fr\n",
      "      [8] alim_nom_sci\n",
      "      [9] Energie, RÃ¨glement UE NÂ° 1169/2011 (kJ/100 g)\n",
      "      [10] Energie, RÃ¨glement UE NÂ° 1169/2011 (kcal/100 g)\n",
      "      [11] Energie, N x facteur Jones, avec fibres  (kJ/100 g)\n",
      "      [12] Energie, N x facteur Jones, avec fibres  (kcal/100 g)\n",
      "      [13] Eau (g/100 g)\n",
      "      [14] ProtÃ©ines, N x facteur de Jones (g/100 g)\n",
      "   âœ… 500 descriptions crÃ©Ã©es\n",
      "\n",
      "   ğŸ“„ Exemple de description :\n",
      "      **0** (pour 100g) : ProtÃ©ines 4,63g, ProtÃ©ines 4,61g, Glucides 36,6g, Lipides 12,9g.\n",
      "\n",
      "âœ… Texte CIQUAL gÃ©nÃ©rÃ© : 68253 caractÃ¨res\n"
     ]
    }
   ],
   "source": [
    "def process_ciqual_data(excel_path):\n",
    "    \"\"\"Traite la table CIQUAL et crÃ©e des descriptions textuelles enrichies\"\"\"\n",
    "    print(\"ğŸ“Š Chargement de la table CIQUAL...\")\n",
    "    \n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    print(f\"   âœ… {len(df)} aliments chargÃ©s\")\n",
    "    print(f\"   ğŸ“‹ Colonnes disponibles : {len(df.columns)}\")\n",
    "    print(f\"   ğŸ” AperÃ§u des colonnes :\")\n",
    "    for i, col in enumerate(df.columns[:15]):\n",
    "        print(f\"      [{i}] {col}\")\n",
    "    \n",
    "    food_descriptions = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx >= 500:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            nom_aliment = str(row.iloc[1]) if len(row) > 1 else \"Aliment inconnu\"\n",
    "            description = f\"**{nom_aliment}** (pour 100g) : \"\n",
    "            \n",
    "            nutriments = []\n",
    "            for col_idx, col_name in enumerate(df.columns):\n",
    "                col_name_lower = str(col_name).lower()\n",
    "                \n",
    "                if 'protÃ©ine' in col_name_lower or 'protein' in col_name_lower:\n",
    "                    val = row.iloc[col_idx]\n",
    "                    if pd.notna(val) and val != '-':\n",
    "                        nutriments.append(f\"ProtÃ©ines {val}g\")\n",
    "                \n",
    "                elif 'lipide' in col_name_lower or 'graisse' in col_name_lower or 'fat' in col_name_lower:\n",
    "                    val = row.iloc[col_idx]\n",
    "                    if pd.notna(val) and val != '-':\n",
    "                        nutriments.append(f\"Lipides {val}g\")\n",
    "                \n",
    "                elif 'glucide' in col_name_lower or 'carb' in col_name_lower:\n",
    "                    val = row.iloc[col_idx]\n",
    "                    if pd.notna(val) and val != '-':\n",
    "                        nutriments.append(f\"Glucides {val}g\")\n",
    "                \n",
    "                elif 'Ã©nergie' in col_name_lower or 'energie' in col_name_lower or 'kcal' in col_name_lower:\n",
    "                    val = row.iloc[col_idx]\n",
    "                    if pd.notna(val) and val != '-':\n",
    "                        nutriments.append(f\"Ã‰nergie {val}kcal\")\n",
    "            \n",
    "            if nutriments:\n",
    "                description += \", \".join(nutriments)\n",
    "            else:\n",
    "                description += \"Composition nutritionnelle disponible\"\n",
    "            \n",
    "            description += \".\"\n",
    "            food_descriptions.append(description)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ… {len(food_descriptions)} descriptions crÃ©Ã©es\")\n",
    "    \n",
    "    if food_descriptions:\n",
    "        print(f\"\\n   ğŸ“„ Exemple de description :\")\n",
    "        print(f\"      {food_descriptions[0]}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(food_descriptions), df\n",
    "\n",
    "\n",
    "excel_path = DATA_DIR / EXCEL_FILE\n",
    "\n",
    "if excel_path.exists():\n",
    "    ciqual_text, ciqual_df = process_ciqual_data(excel_path)\n",
    "    print(f\"\\nâœ… Texte CIQUAL gÃ©nÃ©rÃ© : {len(ciqual_text)} caractÃ¨res\")\n",
    "else:\n",
    "    print(f\"âŒ Fichier CIQUAL introuvable : {EXCEL_FILE}\")\n",
    "    ciqual_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Ã‰TAPE 5 : DÃ©coupage en Chunks AVEC METADATA ENRICHIES\n",
    "\n",
    "### ğŸ”„ Changements majeurs :\n",
    "\n",
    "1. **Chaque chunk hÃ©rite des metadata du PDF parent** :\n",
    "   - Auteurs\n",
    "   - AnnÃ©e\n",
    "   - Journal\n",
    "\n",
    "2. **Metadata spÃ©cifiques au chunk** :\n",
    "   - `page` : NumÃ©ro de page exact\n",
    "   - `section` : Introduction / Methods / Results\n",
    "\n",
    "### ğŸ“Š Structure finale d'un chunk :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"text\": \"contenu du chunk...\",\n",
    "    \"source\": \"helms.pdf\",\n",
    "    \"chunk_id\": 42,\n",
    "    \"type\": \"scientific_paper\",\n",
    "    \n",
    "    # Metadata du PDF (hÃ©ritÃ©)\n",
    "    \"authors\": \"Eric Helms, Alan Aragon\",\n",
    "    \"year\": 2014,\n",
    "    \"journal\": \"Journal of ISSN\",\n",
    "    \"title\": \"Evidence-based...\",\n",
    "    \n",
    "    # Metadata du chunk (spÃ©cifique)\n",
    "    \"page\": 12,\n",
    "    \"section\": \"methods\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ DÃ©coupage des PDFs en chunks avec metadata enrichies...\n",
      "\n",
      "ğŸ“„ helms_bodybuilding_nutrition.pdf: 180 chunks\n",
      "   â†’ Auteurs: ['Open A', 'Eric R']\n",
      "   â†’ AnnÃ©e: 2014\n",
      "ğŸ“„ issn_protein_position.pdf: 61 chunks\n",
      "   â†’ Auteurs: ['Med C', 'International S']\n",
      "   â†’ AnnÃ©e: 2015\n",
      "ğŸ“„ schoenfeld_rom_hypertrophy.pdf: 1003 chunks\n",
      "   â†’ Auteurs: ['Brad Schoenfeld']\n",
      "   â†’ AnnÃ©e: 2016\n",
      "ğŸ“„ bernardez_training_variables.pdf: 106 chunks\n",
      "   â†’ Auteurs: ['Javier Raya-GonzaÌlez']\n",
      "   â†’ AnnÃ©e: 2022\n",
      "\n",
      "âœ‚ï¸ DÃ©coupage de la table CIQUAL...\n",
      "   ğŸ“Š Table CIQUAL: 88 chunks\n",
      "\n",
      "âœ… TOTAL : 1438 chunks crÃ©Ã©s avec metadata enrichies\n",
      "\n",
      "ğŸ“„ EXEMPLE DE CHUNK ENRICHI:\n",
      "   Source: helms_bodybuilding_nutrition.pdf\n",
      "   Auteurs: Open A, Eric R, Alan A\n",
      "   AnnÃ©e: 2014\n",
      "   Journal: Unknown Journal...\n",
      "   Page: 2\n",
      "   Section: body\n",
      "   Texte (100 premiers chars): ied however; non-overweight men who consumed 50%\n",
      "of their maintenance caloric intake for 24 weeks an...\n"
     ]
    }
   ],
   "source": [
    "# Configuration text splitter (identique Ã  V1)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# DÃ©coupage des PDFs AVEC metadata\n",
    "print(\"âœ‚ï¸ DÃ©coupage des PDFs en chunks avec metadata enrichies...\\n\")\n",
    "\n",
    "for pdf_name, pdf_data in pdf_contents_with_meta.items():\n",
    "    pdf_text = pdf_data['text']\n",
    "    pdf_meta = pdf_data['metadata']\n",
    "    \n",
    "    # DÃ©couper en chunks\n",
    "    chunks = text_splitter.split_text(pdf_text)\n",
    "    \n",
    "    # Ouvrir le PDF pour estimation pages\n",
    "    pdf_path = DATA_DIR / pdf_name\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Estimer page du chunk\n",
    "        page_num = estimate_page_number(doc, chunk, i, len(chunks))\n",
    "        \n",
    "        # DÃ©tecter section\n",
    "        section = detect_section(chunk)\n",
    "        \n",
    "        # CrÃ©er chunk enrichi\n",
    "        # DIFFÃ‰RENCE V1 vs V2 : On ajoute authors, year, journal, page, section\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": pdf_name,\n",
    "            \"chunk_id\": i,\n",
    "            \"type\": \"scientific_paper\",\n",
    "            \n",
    "            # METADATA ENRICHIES (NOUVEAU)\n",
    "            \"authors\": \", \".join(pdf_meta['authors'][:3]),  # String pour ChromaDB\n",
    "            \"year\": pdf_meta['year'],\n",
    "            \"journal\": pdf_meta['journal'],\n",
    "            \"title\": pdf_meta['title'],\n",
    "            \"page\": page_num,\n",
    "            \"section\": section,\n",
    "            \"total_pages\": pdf_meta['total_pages']\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"ğŸ“„ {pdf_name}: {len(chunks)} chunks\")\n",
    "    print(f\"   â†’ Auteurs: {pdf_meta['authors'][:2]}\")\n",
    "    print(f\"   â†’ AnnÃ©e: {pdf_meta['year']}\")\n",
    "\n",
    "# DÃ©coupage CIQUAL (inchangÃ©)\n",
    "if ciqual_text:\n",
    "    print(\"\\nâœ‚ï¸ DÃ©coupage de la table CIQUAL...\")\n",
    "    ciqual_chunks = text_splitter.split_text(ciqual_text)\n",
    "    \n",
    "    for i, chunk in enumerate(ciqual_chunks):\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": EXCEL_FILE,\n",
    "            \"chunk_id\": i,\n",
    "            \"type\": \"nutritional_data\",\n",
    "            # Metadata standards pour CIQUAL\n",
    "            \"authors\": \"ANSES\",\n",
    "            \"year\": 2020,\n",
    "            \"journal\": \"Government Database\",\n",
    "            \"title\": \"CIQUAL Nutritional Database\",\n",
    "            \"page\": None,\n",
    "            \"section\": \"data\",\n",
    "            \"total_pages\": None\n",
    "        })\n",
    "    \n",
    "    print(f\"   ğŸ“Š Table CIQUAL: {len(ciqual_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nâœ… TOTAL : {len(all_chunks)} chunks crÃ©Ã©s avec metadata enrichies\")\n",
    "\n",
    "# Afficher exemple de chunk enrichi\n",
    "print(\"\\nğŸ“„ EXEMPLE DE CHUNK ENRICHI:\")\n",
    "example = all_chunks[10]  # Chunk au milieu\n",
    "print(f\"   Source: {example['source']}\")\n",
    "print(f\"   Auteurs: {example['authors']}\")\n",
    "print(f\"   AnnÃ©e: {example['year']}\")\n",
    "print(f\"   Journal: {example['journal'][:40]}...\")\n",
    "print(f\"   Page: {example['page']}\")\n",
    "print(f\"   Section: {example['section']}\")\n",
    "print(f\"   Texte (100 premiers chars): {example['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Ã‰TAPE 6 : GÃ©nÃ©ration des Embeddings (InchangÃ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Chargement du modÃ¨le d'embeddings...\n",
      "   ğŸ“¥ TÃ©lÃ©chargement bge-large-en-v1.5 (~1.34 GB, 2-3 min)...\n",
      "âœ… ModÃ¨le chargÃ© - Dimension : 1024\n",
      "\n",
      "ğŸ”„ GÃ©nÃ©ration des embeddings pour 1438 chunks...\n",
      "   â³ Cela peut prendre quelques minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dbfb1466da480e9d08747cd54572e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Embeddings gÃ©nÃ©rÃ©s : (1438, 1024)\n",
      "   Dimension des vecteurs : 1024\n"
     ]
    }
   ],
   "source": [
    "# Charger le modÃ¨le d'embeddings\n",
    "print(\"ğŸ§  Chargement du modÃ¨le d'embeddings...\")\n",
    "print(\"   ğŸ“¥ TÃ©lÃ©chargement bge-large-en-v1.5 (~1.34 GB, 2-3 min)...\")\n",
    "embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "##embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ… ModÃ¨le chargÃ© - Dimension : {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# GÃ©nÃ©rer les embeddings\n",
    "print(f\"\\nğŸ”„ GÃ©nÃ©ration des embeddings pour {len(all_chunks)} chunks...\")\n",
    "print(\"   â³ Cela peut prendre quelques minutes...\")\n",
    "\n",
    "texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… Embeddings gÃ©nÃ©rÃ©s : {embeddings.shape}\")\n",
    "print(f\"   Dimension des vecteurs : {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Ã‰TAPE 7 : Stockage dans ChromaDB AVEC METADATA ENRICHIES\n",
    "\n",
    "### ğŸ”„ DiffÃ©rence majeure avec V1 :\n",
    "\n",
    "**AVANT (V1)** - Metadata basiques :\n",
    "```python\n",
    "metadatas = [{\n",
    "    \"source\": \"helms.pdf\",\n",
    "    \"chunk_id\": \"42\",\n",
    "    \"type\": \"scientific_paper\"\n",
    "}]\n",
    "```\n",
    "\n",
    "**APRÃˆS (V2)** - Metadata enrichies :\n",
    "```python\n",
    "metadatas = [{\n",
    "    \"source\": \"helms.pdf\",\n",
    "    \"chunk_id\": \"42\",\n",
    "    \"type\": \"scientific_paper\",\n",
    "    \"authors\": \"Eric Helms, Alan Aragon\",  # NOUVEAU\n",
    "    \"year\": \"2014\",                         # NOUVEAU\n",
    "    \"journal\": \"Journal of ISSN\",          # NOUVEAU\n",
    "    \"page\": \"12\",                           # NOUVEAU\n",
    "    \"section\": \"methods\"                    # NOUVEAU\n",
    "}]\n",
    "```\n",
    "\n",
    "### âš ï¸ Note importante ChromaDB :\n",
    "- ChromaDB n'accepte QUE des strings en metadata\n",
    "- On convertit `year` (int) â†’ `str(year)`\n",
    "- On exclut les valeurs `None` (ChromaDB crash sinon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Initialisation de ChromaDB...\n",
      "âœ… Collection 'fitness_knowledge_base' crÃ©Ã©e\n",
      "\n",
      "ğŸ“¥ Ajout de 1438 documents Ã  ChromaDB...\n",
      "   âœ… Batch 1/15 ajoutÃ©\n",
      "   âœ… Batch 2/15 ajoutÃ©\n",
      "   âœ… Batch 3/15 ajoutÃ©\n",
      "   âœ… Batch 4/15 ajoutÃ©\n",
      "   âœ… Batch 5/15 ajoutÃ©\n",
      "   âœ… Batch 6/15 ajoutÃ©\n",
      "   âœ… Batch 7/15 ajoutÃ©\n",
      "   âœ… Batch 8/15 ajoutÃ©\n",
      "   âœ… Batch 9/15 ajoutÃ©\n",
      "   âœ… Batch 10/15 ajoutÃ©\n",
      "   âœ… Batch 11/15 ajoutÃ©\n",
      "   âœ… Batch 12/15 ajoutÃ©\n",
      "   âœ… Batch 13/15 ajoutÃ©\n",
      "   âœ… Batch 14/15 ajoutÃ©\n",
      "   âœ… Batch 15/15 ajoutÃ©\n",
      "\n",
      "âœ… Tous les documents stockÃ©s dans ChromaDB avec metadata enrichies\n",
      "   ğŸ“Š Total : 1438 documents\n",
      "\n",
      "ğŸ” VÃ©rification metadata stockÃ©es:\n",
      "   Exemple metadata chunk_10:\n",
      "      page: 2\n",
      "      authors: Open A, Eric R, Alan A\n",
      "      journal: Unknown Journal\n",
      "      title: Evidence-based recommendations for natural bodybuilding contest preparation: nutrition and supplementation\n",
      "      type: scientific_paper\n",
      "      year: 2014\n",
      "      chunk_id: 10\n",
      "      section: body\n",
      "      source: helms_bodybuilding_nutrition.pdf\n"
     ]
    }
   ],
   "source": [
    "# Initialiser ChromaDB\n",
    "print(\"ğŸ’¾ Initialisation de ChromaDB...\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMA_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "collection_name = \"fitness_knowledge_base\"\n",
    "\n",
    "# Supprimer ancienne collection\n",
    "try:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "    print(f\"   ğŸ—‘ï¸ Ancienne collection supprimÃ©e\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"Base de connaissances fitness : nutrition, hypertrophie, biomÃ©canique\"}\n",
    ")\n",
    "\n",
    "print(f\"âœ… Collection '{collection_name}' crÃ©Ã©e\")\n",
    "\n",
    "# PrÃ©parer les donnÃ©es AVEC metadata enrichies\n",
    "print(f\"\\nğŸ“¥ Ajout de {len(all_chunks)} documents Ã  ChromaDB...\")\n",
    "\n",
    "ids = [f\"chunk_{i}\" for i in range(len(all_chunks))]\n",
    "documents = [chunk[\"text\"] for chunk in all_chunks]\n",
    "\n",
    "# Metadata enrichies pour ChromaDB\n",
    "metadatas = []\n",
    "for chunk in all_chunks:\n",
    "    meta = {\n",
    "        \"source\": chunk[\"source\"],\n",
    "        \"chunk_id\": str(chunk[\"chunk_id\"]),\n",
    "        \"type\": chunk[\"type\"],\n",
    "        \n",
    "        # NOUVELLES METADATA\n",
    "        \"authors\": chunk[\"authors\"],\n",
    "        \"journal\": chunk[\"journal\"],\n",
    "        \"title\": chunk[\"title\"],\n",
    "        \"section\": chunk[\"section\"]\n",
    "    }\n",
    "    \n",
    "    # Ajouter year et page si disponibles (ChromaDB n'aime pas None)\n",
    "    if chunk[\"year\"] is not None:\n",
    "        meta[\"year\"] = str(chunk[\"year\"])  # int â†’ string\n",
    "    \n",
    "    if chunk[\"page\"] is not None:\n",
    "        meta[\"page\"] = str(chunk[\"page\"])  # int â†’ string\n",
    "    \n",
    "    metadatas.append(meta)\n",
    "\n",
    "# Ajouter par batch\n",
    "batch_size = 100\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    batch_end = min(i + batch_size, len(ids))\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[i:batch_end],\n",
    "        documents=documents[i:batch_end],\n",
    "        metadatas=metadatas[i:batch_end],\n",
    "        embeddings=embeddings[i:batch_end].tolist()\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Batch {i//batch_size + 1}/{(len(ids)-1)//batch_size + 1} ajoutÃ©\")\n",
    "\n",
    "print(f\"\\nâœ… Tous les documents stockÃ©s dans ChromaDB avec metadata enrichies\")\n",
    "print(f\"   ğŸ“Š Total : {collection.count()} documents\")\n",
    "\n",
    "# VÃ©rifier metadata stockÃ©es\n",
    "print(\"\\nğŸ” VÃ©rification metadata stockÃ©es:\")\n",
    "sample_result = collection.get(ids=[\"chunk_10\"], include=[\"metadatas\"])\n",
    "sample_meta = sample_result['metadatas'][0]\n",
    "print(f\"   Exemple metadata chunk_10:\")\n",
    "for key, value in sample_meta.items():\n",
    "    print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Ã‰TAPE 8 : Tests de RequÃªtes AVEC FILTRES METADATA\n",
    "\n",
    "### ğŸ†• Nouvelles fonctionnalitÃ©s :\n",
    "\n",
    "1. **Filtrage par type** : `doc_type=\"scientific_paper\"`\n",
    "2. **Filtrage temporel** : `min_year=2020` â† â­ APPORT MAJEUR selon ta fiche\n",
    "3. **Affichage metadata enrichies** : Auteurs, annÃ©e, journal, page, section\n",
    "\n",
    "### ğŸ”— Lien avec ta Fiche RAG - PHASE 1 :\n",
    "\n",
    "**Citation** :\n",
    "> *\"Crucial pour le filtrage temporel (Ã©viter la 'Stale Data')\"*\n",
    "\n",
    "**Application concrÃ¨te** :\n",
    "```python\n",
    "# Chercher seulement Ã©tudes rÃ©centes (>= 2020)\n",
    "search_knowledge_base(\n",
    "    \"muscle hypertrophy\",\n",
    "    min_year=2020  # â† Ã‰vite Ã©tudes obsolÃ¨tes\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§ª TESTS AVEC BGE-LARGE EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "### TEST 1 : Recommandations ProtÃ©ines ###\n",
      "\n",
      "ğŸ” RequÃªte : 'protein intake recommendations resistance training hypertrophy grams per kilogram'\n",
      "   ğŸ¯ Filtres : type=scientific_paper\n",
      "   ğŸ“Š RÃ©sultats : 3/3\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: schoenfeld_rom_hypertrophy.pdf\n",
      "âœï¸  Auteurs: Brad Schoenfeld\n",
      "ğŸ“… AnnÃ©e: 2016\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 194\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.6283\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "tissue accretion and the repair of exercise-induced muscle damage (118). The doseâ€“response\n",
      "relationship between protein intake and hypertrophy appears to top out at approximately 2.0 g/kg/day\n",
      "(118); consuming substantially larger amounts of dietary protein does not result in further increases\n",
      "in lea...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: issn_protein_position.pdf\n",
      "âœï¸  Auteurs: Med C, International S, Sports N\n",
      "ğŸ“… AnnÃ©e: 2015\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 2\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.6021\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "within the body for protein intakes at the upper end of\n",
      "this range. Strength/power exercise is thought to increase\n",
      "protein requirements even more than endurance exercise,\n",
      "particularly during the initial stages of training and/or\n",
      "sharp increases in volume. Recommendations for\n",
      "strength/power exercise ...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: helms_bodybuilding_nutrition.pdf\n",
      "âœï¸  Auteurs: Open A, Eric R, Alan A\n",
      "ğŸ“… AnnÃ©e: 2014\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 15\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.5999\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "35.\n",
      "Phillips SM: Protein requirements and supplementation in strength\n",
      "sports. Nutrition 2004, 20:689â€“695.\n",
      "36.\n",
      "Tarnopolsky MA: Building muscle: nutrition to maximize bulk and\n",
      "strength adaptations to resistance exercise training. Eur J Sport Sci 2008,\n",
      "8:67â€“76.\n",
      "37.\n",
      "Tipton KD: Protein for adaptations to...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "### TEST 2 : Volume EntraÃ®nement (Ã‰tudes RÃ©centes >= 2020) ###\n",
      "\n",
      "ğŸ” RequÃªte : 'weekly training volume sets per muscle group hypertrophy dose-response'\n",
      "   ğŸ¯ Filtres : type=scientific_paper, annÃ©e>=2020\n",
      "   ğŸ“Š RÃ©sultats : 3/3\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: bernardez_training_variables.pdf\n",
      "âœï¸  Auteurs: Javier Raya-GonzaÌlez\n",
      "ğŸ“… AnnÃ©e: 2022\n",
      "ğŸ“° Journal: Frontiers in Sports...\n",
      "ğŸ“– Page: 10\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.6895\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "of the resistance training program with the aim of optimizing\n",
      "muscle hypertrophy.\n",
      "PRACTICAL APPLICATIONS\n",
      "From the existing literature some recommendations must be\n",
      "considered when resistance training program focused on muscle\n",
      "mass gains are prescribed:\n",
      "a) Volume: research has reported a graded dose-r...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: bernardez_training_variables.pdf\n",
      "âœï¸  Auteurs: Javier Raya-GonzaÌlez\n",
      "ğŸ“… AnnÃ©e: 2022\n",
      "ğŸ“° Journal: Frontiers in Sports...\n",
      "ğŸ“– Page: 6\n",
      "ğŸ·ï¸  Section: results\n",
      "ğŸ¯ Score: 0.6866\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "multiple sets (2â€“3 sets) entails 40% more hypertrophy compared\n",
      "to a single set (Krieger, 2010). Regarding the analysis of the dose-\n",
      "response relationship, the results of this meta-analysis (Krieger,\n",
      "2010) reported signiï¬cant diï¬€erences in muscle mass gains when\n",
      "2-3 sets compared to 1 set were perfor...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: bernardez_training_variables.pdf\n",
      "âœï¸  Auteurs: Javier Raya-GonzaÌlez\n",
      "ğŸ“… AnnÃ©e: 2022\n",
      "ğŸ“° Journal: Frontiers in Sports...\n",
      "ğŸ“– Page: 6\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.6755\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "established that low-volume protocols (â‰¤4 weekly sets per muscle\n",
      "group) could be enough to get substantial gains in muscle\n",
      "hypertrophy, which is valuable information to those for which\n",
      "the conservation of energy is an ongoing concern or those\n",
      "with a reduced time availability. However, they also obse...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "### TEST 3 : Range of Motion ###\n",
      "\n",
      "ğŸ” RequÃªte : 'range of motion full ROM partial ROM muscle hypertrophy development'\n",
      "   ğŸ¯ Filtres : type=scientific_paper\n",
      "   ğŸ“Š RÃ©sultats : 3/3\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: schoenfeld_rom_hypertrophy.pdf\n",
      "âœï¸  Auteurs: Brad Schoenfeld\n",
      "ğŸ“… AnnÃ©e: 2016\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 107\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.6752\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "--- PAGE 122 ---\n",
      "\n",
      "Range of Motion\n",
      "Practical Applications\n",
      "Maximal muscle development requires training through a complete ROM. Thus, full ROM\n",
      "movements should form the basis of a hypertrophy-oriented program. The stretched position\n",
      "appears particularly important to elicit hypertrophic gains. That sai...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: schoenfeld_rom_hypertrophy.pdf\n",
      "âœï¸  Auteurs: Brad Schoenfeld\n",
      "ğŸ“… AnnÃ©e: 2016\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 121\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.5885\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "benefit to training through a full ROM. This has been displayed in both upper- and lower-body\n",
      "muscles using a variety of exercises. Pinto and colleagues (581) showed that full ROM training of the\n",
      "elbow flexors (0 to 130Â° of flexion) produced greater increases in muscle thickness compared to\n",
      "partial-...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ RÃ‰SULTAT 3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“š Source: schoenfeld_rom_hypertrophy.pdf\n",
      "âœï¸  Auteurs: Brad Schoenfeld\n",
      "ğŸ“… AnnÃ©e: 2016\n",
      "ğŸ“° Journal: Unknown Journal...\n",
      "ğŸ“– Page: 127\n",
      "ğŸ·ï¸  Section: body\n",
      "ğŸ¯ Score: 0.5620\n",
      "\n",
      "ğŸ’¬ Extrait:\n",
      "range (6- to 12RM) and devoting specific training cycles to lower- and higher-repetition\n",
      "training.\n",
      "Once facility has been established with the basic movement patterns, a variety of exercises\n",
      "should be employed over the course of a periodized training program to maximize whole-body\n",
      "muscle hypertrophy...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'documents': [['--- PAGE 122 ---\\n\\nRange of Motion\\nPractical Applications\\nMaximal muscle development requires training through a complete ROM. Thus, full ROM\\nmovements should form the basis of a hypertrophy-oriented program. The stretched position\\nappears particularly important to elicit hypertrophic gains. That said, integrating some partial-\\nrange movements may enhance hypertrophy.\\n\\n\\n--- PAGE 123 ---',\n",
       "   'benefit to training through a full ROM. This has been displayed in both upper- and lower-body\\nmuscles using a variety of exercises. Pinto and colleagues (581) showed that full ROM training of the\\nelbow flexors (0 to 130Â° of flexion) produced greater increases in muscle thickness compared to\\npartial-range training (50 to 100Â° of flexion). The difference in effect size strongly favored the full\\nROM condition (1.09 vs. 0.57, respectively), indicating that the magnitude of variance was\\nmeaningful. Similarly, McMahon and colleagues (482) showed that although knee extension at full\\nROM (0 to 90Â°) and partial ROM (0 to 50Â°) both increased quadriceps muscle cross-sectional area,\\nthe magnitude of hypertrophy was significantly greater at 75% of femur length in the full-range\\ncondition. Interestingly, Bloomquist and colleagues (81) showed that deep squats (0 to 120Â° of knee\\nflexion) promoted increases in cross-sectional area across the entire frontal thigh musculature,',\n",
       "   'range (6- to 12RM) and devoting specific training cycles to lower- and higher-repetition\\ntraining.\\nOnce facility has been established with the basic movement patterns, a variety of exercises\\nshould be employed over the course of a periodized training program to maximize whole-body\\nmuscle hypertrophy. This should include the liberal use of free-form (i.e., free weights and\\ncables) and machine-based exercises. Similarly, both multi- and single-joint exercises should be\\nincluded in a hypertrophy-specific routine to maximize muscular growth.\\nBoth concentric and eccentric actions should be incorporated during training. Evidence of the\\nbenefits of combining isometric actions with dynamic actions is lacking at this time. The\\naddition of supramaximal eccentric loading may enhance the hypertrophic response.\\nAn optimal rest interval for hypertrophy training does not appear to exist. Research indicates that']],\n",
       " 'metadatas': [[{'journal': 'Unknown Journal',\n",
       "    'page': '107',\n",
       "    'authors': 'Brad Schoenfeld',\n",
       "    'title': 'Science and Development of Muscle Hypertrophy',\n",
       "    'chunk_id': '410',\n",
       "    'section': 'body',\n",
       "    'source': 'schoenfeld_rom_hypertrophy.pdf',\n",
       "    'type': 'scientific_paper',\n",
       "    'year': '2016'},\n",
       "   {'journal': 'Unknown Journal',\n",
       "    'source': 'schoenfeld_rom_hypertrophy.pdf',\n",
       "    'page': '121',\n",
       "    'authors': 'Brad Schoenfeld',\n",
       "    'section': 'body',\n",
       "    'title': 'Science and Development of Muscle Hypertrophy',\n",
       "    'chunk_id': '407',\n",
       "    'type': 'scientific_paper',\n",
       "    'year': '2016'},\n",
       "   {'page': '127',\n",
       "    'source': 'schoenfeld_rom_hypertrophy.pdf',\n",
       "    'type': 'scientific_paper',\n",
       "    'journal': 'Unknown Journal',\n",
       "    'section': 'body',\n",
       "    'authors': 'Brad Schoenfeld',\n",
       "    'year': '2016',\n",
       "    'title': 'Science and Development of Muscle Hypertrophy',\n",
       "    'chunk_id': '424'}]],\n",
       " 'distances': [[0.3247517943382263, 0.4114823639392853, 0.43800726532936096]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 8 : Recherche avec Filtrage Post-Recherche + BGE-large\n",
    "# ============================================================================\n",
    "\n",
    "def search_knowledge_base(query, n_results=5, doc_type=None, min_year=None):\n",
    "    \"\"\"\n",
    "    Recherche avec filtrage post-recherche\n",
    "    IMPORTANT : Utilise le MÃŠME modÃ¨le d'embedding que l'indexation\n",
    "    \"\"\"\n",
    "    # ========================================================================\n",
    "    # Ã‰TAPE 1 : ENCODER LA QUERY avec BGE-large\n",
    "    # ========================================================================\n",
    "    \n",
    "    # âš ï¸ CRITIQUE : Utiliser le MÃŠME modÃ¨le que Cellule 6\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Ã‰TAPE 2 : RECHERCHE LARGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    search_size = n_results * 3\n",
    "    \n",
    "    where_filter = None\n",
    "    if doc_type:\n",
    "        where_filter = {\"type\": doc_type}\n",
    "    \n",
    "    # Recherche avec embedding prÃ©-calculÃ©\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # â† Pas query_texts !\n",
    "        n_results=search_size,\n",
    "        where=where_filter,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Ã‰TAPE 3 : FILTRAGE POST-RECHERCHE\n",
    "    # ========================================================================\n",
    "    \n",
    "    filtered_results = {\n",
    "        'documents': [[]],\n",
    "        'metadatas': [[]],\n",
    "        'distances': [[]]\n",
    "    }\n",
    "    \n",
    "    for doc, meta, dist in zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ):\n",
    "        # Filtre temporel\n",
    "        if min_year:\n",
    "            year_str = meta.get('year')\n",
    "            if year_str:\n",
    "                try:\n",
    "                    year = int(year_str)\n",
    "                    if year < min_year:\n",
    "                        continue\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        filtered_results['documents'][0].append(doc)\n",
    "        filtered_results['metadatas'][0].append(meta)\n",
    "        filtered_results['distances'][0].append(dist)\n",
    "        \n",
    "        if len(filtered_results['documents'][0]) >= n_results:\n",
    "            break\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Ã‰TAPE 4 : AFFICHAGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\nğŸ” RequÃªte : '{query}'\")\n",
    "    \n",
    "    active_filters = []\n",
    "    if doc_type:\n",
    "        active_filters.append(f\"type={doc_type}\")\n",
    "    if min_year:\n",
    "        active_filters.append(f\"annÃ©e>={min_year}\")\n",
    "    \n",
    "    if active_filters:\n",
    "        print(f\"   ğŸ¯ Filtres : {', '.join(active_filters)}\")\n",
    "    \n",
    "    print(f\"   ğŸ“Š RÃ©sultats : {len(filtered_results['documents'][0])}/{n_results}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        filtered_results['documents'][0],\n",
    "        filtered_results['metadatas'][0],\n",
    "        filtered_results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\nğŸ“„ RÃ‰SULTAT {i}\")\n",
    "        print(f\"â”€\" * 80)\n",
    "        \n",
    "        print(f\"ğŸ“š Source: {metadata.get('source', 'N/A')}\")\n",
    "        print(f\"âœï¸  Auteurs: {metadata.get('authors', 'N/A')}\")\n",
    "        print(f\"ğŸ“… AnnÃ©e: {metadata.get('year', 'N/A')}\")\n",
    "        print(f\"ğŸ“° Journal: {metadata.get('journal', 'N/A')[:50]}...\")\n",
    "        print(f\"ğŸ“– Page: {metadata.get('page', 'N/A')}\")\n",
    "        print(f\"ğŸ·ï¸  Section: {metadata.get('section', 'N/A')}\")\n",
    "        print(f\"ğŸ¯ Score: {1 - distance:.4f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¬ Extrait:\")\n",
    "        print(f\"{doc[:300]}...\")\n",
    "        print(\"â”€\" * 80)\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª TESTS AVEC BGE-LARGE EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1 : ProtÃ©ines\n",
    "print(\"\\n\\n### TEST 1 : Recommandations ProtÃ©ines ###\")\n",
    "search_knowledge_base(\n",
    "    \"protein intake recommendations resistance training hypertrophy grams per kilogram\",\n",
    "    n_results=3,\n",
    "    doc_type=\"scientific_paper\"\n",
    ")\n",
    "\n",
    "# Test 2 : Volume (AVEC FILTRE COMBINÃ‰)\n",
    "print(\"\\n\\n### TEST 2 : Volume EntraÃ®nement (Ã‰tudes RÃ©centes >= 2020) ###\")\n",
    "search_knowledge_base(\n",
    "    \"weekly training volume sets per muscle group hypertrophy dose-response\",\n",
    "    n_results=3,\n",
    "    doc_type=\"scientific_paper\",\n",
    "    min_year=2020\n",
    ")\n",
    "\n",
    "# Test 3 : ROM\n",
    "print(\"\\n\\n### TEST 3 : Range of Motion ###\")\n",
    "search_knowledge_base(\n",
    "    \"range of motion full ROM partial ROM muscle hypertrophy development\",\n",
    "    n_results=3,\n",
    "    doc_type=\"scientific_paper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ã‰TAPE 9 : Statistiques Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š STATISTIQUES DE LA BASE DE CONNAISSANCES V2\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Documents sources :\n",
      "   - PDFs scientifiques : 4\n",
      "   - Table nutritionnelle : 1 (CIQUAL)\n",
      "\n",
      "ğŸ“ Chunks :\n",
      "   - Total : 1438\n",
      "   - Taille moyenne : 747 caractÃ¨res\n",
      "\n",
      "ğŸ’¾ ChromaDB :\n",
      "   - Collection : fitness_knowledge_base\n",
      "   - Documents indexÃ©s : 1438\n",
      "   - Emplacement : C:\\RAG-Fitness-Test\\data\\processed\\chroma_db\n",
      "\n",
      "ğŸ§  ModÃ¨le d'embeddings :\n",
      "   - ModÃ¨le : all-MiniLM-L6-v2\n",
      "   - Dimension : 1024\n",
      "\n",
      "ğŸ†• METADATA ENRICHIES (V2) :\n",
      "   - Auteurs : âœ… Extraits automatiquement\n",
      "   - AnnÃ©e : âœ… Filtrage temporel activÃ©\n",
      "   - Journal : âœ… CrÃ©dibilitÃ© source\n",
      "   - Page : âœ… Citation prÃ©cise\n",
      "   - Section : âœ… Contextualisation\n",
      "\n",
      "================================================================================\n",
      "âœ… PIPELINE D'INGESTION V2 TERMINÃ‰ AVEC SUCCÃˆS !\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Prochaine Ã©tape : ItÃ©ration 4 - Changer modÃ¨le embedding (BGE-large)\n",
      "ğŸ“Œ Puis : ItÃ©ration 5 - Hybrid Search + Re-ranking\n",
      "ğŸ“Œ Enfin : ItÃ©ration 6 - Chatbot Gradio + Ollama\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š STATISTIQUES DE LA BASE DE CONNAISSANCES V2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“š Documents sources :\")\n",
    "print(f\"   - PDFs scientifiques : {len(PDF_FILES)}\")\n",
    "print(f\"   - Table nutritionnelle : 1 (CIQUAL)\")\n",
    "\n",
    "print(f\"\\nğŸ“ Chunks :\")\n",
    "print(f\"   - Total : {len(all_chunks)}\")\n",
    "print(f\"   - Taille moyenne : {sum(len(c['text']) for c in all_chunks) // len(all_chunks)} caractÃ¨res\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ChromaDB :\")\n",
    "print(f\"   - Collection : {collection_name}\")\n",
    "print(f\"   - Documents indexÃ©s : {collection.count()}\")\n",
    "print(f\"   - Emplacement : {CHROMA_DIR}\")\n",
    "\n",
    "print(f\"\\nğŸ§  ModÃ¨le d'embeddings :\")\n",
    "print(f\"   - ModÃ¨le : all-MiniLM-L6-v2\")\n",
    "print(f\"   - Dimension : {embeddings.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ†• METADATA ENRICHIES (V2) :\")\n",
    "print(f\"   - Auteurs : âœ… Extraits automatiquement\")\n",
    "print(f\"   - AnnÃ©e : âœ… Filtrage temporel activÃ©\")\n",
    "print(f\"   - Journal : âœ… CrÃ©dibilitÃ© source\")\n",
    "print(f\"   - Page : âœ… Citation prÃ©cise\")\n",
    "print(f\"   - Section : âœ… Contextualisation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PIPELINE D'INGESTION V2 TERMINÃ‰ AVEC SUCCÃˆS !\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ“Œ Prochaine Ã©tape : ItÃ©ration 4 - Changer modÃ¨le embedding (BGE-large)\")\n",
    "print(\"ğŸ“Œ Puis : ItÃ©ration 5 - Hybrid Search + Re-ranking\")\n",
    "print(\"ğŸ“Œ Enfin : ItÃ©ration 6 - Chatbot Gradio + Ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ RÃ‰SUMÃ‰ DES AMÃ‰LIORATIONS V2\n",
    "\n",
    "### ğŸ”„ Comparaison V1 vs V2\n",
    "\n",
    "| **Aspect** | **V1 (Basique)** | **V2 (Metadata Enrichies)** |\n",
    "|------------|------------------|-----------------------------|\n",
    "| **Metadata par chunk** | 3 champs | 10 champs (+233%) |\n",
    "| **Filtrage temporel** | âŒ Impossible | âœ… `min_year=2020` |\n",
    "| **Citations** | \"Selon helms.pdf\" | \"Selon Helms et al. (2014), Journal of ISSN, p.12\" |\n",
    "| **Contextualisation** | âŒ Aucune | âœ… Section (Methods/Results) |\n",
    "| **CrÃ©dibilitÃ©** | âŒ Pas vÃ©rifiable | âœ… Journal affichÃ© |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Lien avec ta Fiche RAG\n",
    "\n",
    "**PHASE 1 : Ingestion / Nettoyage (ETL)** âœ… IMPLÃ‰MENTÃ‰\n",
    "\n",
    "> *\"Metadata Extraction : Date, Auteur, Version. Crucial pour le filtrage temporel (Ã©viter la 'Stale Data').\"*\n",
    "\n",
    "**Ce qu'on a fait** :\n",
    "- âœ… Date â†’ AnnÃ©e extraction (2014, 2020, etc.)\n",
    "- âœ… Auteur â†’ Liste auteurs (Helms, Aragon, etc.)\n",
    "- âœ… Version â†’ Journal + Page (contexte)\n",
    "- âœ… Filtrage temporel â†’ `min_year` parameter\n",
    "\n",
    "---\n",
    "\n",
    "**PROCHAINES PHASES Ã  implÃ©menter** :\n",
    "\n",
    "- **PHASE 3** : Embedding (ItÃ©ration 4 - BGE-large)\n",
    "- **PHASE 4** : Retrieval (ItÃ©ration 5 - Hybrid Search + Re-ranking)\n",
    "- **PHASE 5** : GÃ©nÃ©ration (ItÃ©ration 6 - Chatbot)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
